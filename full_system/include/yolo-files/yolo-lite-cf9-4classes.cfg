[net]
# Testing
batch=64
subdivisions=16
# Training
# batch=64 # Changed (from 32)
# subdivisions=16 # Changed (from 1)
width=224 # Unchanged (Could do 416 or any multiple of 32)
height=224 # Unchanged (Could do 416 or any multiple of 32)
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.0002 # Unchanged (Could do 0.001) 
burn_in=1000
max_batches = 800000 # Changed (Could do classes*2000)
policy=steps
steps=400000,450000 # Unchanged (Could do (0.8 & 0.9) * max_batches) 
scales=.1,.1

[convolutional]
batch_normalize=0
filters=16
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=0
filters=32
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=0
filters=64
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=0
filters=128
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2


[convolutional]
batch_normalize=0
filters=128
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

###########

[convolutional]
batch_normalize=0
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=45 # Changed (425 | or (classes + 5)x3 or (classes + 5)x5)
activation=linear

[region]
anchors =  1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52
bias_match=1
classes=4 # Changed (80)
coords=4
num=5
softmax=1
jitter=.2
rescore=0

object_scale=5
noobject_scale=1
class_scale=1
coord_scale=1

absolute=1
thresh = .5
random=1
